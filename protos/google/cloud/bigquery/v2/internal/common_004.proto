// Copyright 2024 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//      https://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// Generated by the C++ microgenerator.
// If you make any local changes, they will be lost.
// file:///workspace/generator/discovery/bigquery_public_google_rest_v2.json
// revision: 20240124

syntax = "proto3";

package google.cloud.cpp.bigquery.v2;

// Options for external data sources.
message AvroOptions {
  // Optional. If sourceFormat is set to "AVRO", indicates whether to interpret
  // logical types as the corresponding BigQuery data type (for example,
  // TIMESTAMP), instead of using the raw type (for example, INTEGER).
  optional bool use_avro_logical_types = 1 [json_name="useAvroLogicalTypes"];
}

// Information related to a Bigtable column.
message BigtableColumn {
  // Optional. The encoding of the values when the type is not STRING.
  // Acceptable encoding values are: TEXT - indicates values are alphanumeric
  // text strings. BINARY - indicates values are encoded using HBase
  // Bytes.toBytes family of functions. 'encoding' can also be set at the column
  // family level. However, the setting at this level takes precedence if
  // 'encoding' is set at both levels.
  optional string encoding = 1 [json_name="encoding"];

  // Optional. If the qualifier is not a valid BigQuery field identifier i.e.
  // does not match a-zA-Z*, a valid identifier must be provided as the column
  // field name and is used as field name in queries.
  optional string field_name = 2 [json_name="fieldName"];

  // Optional. If this is set, only the latest version of value in this column
  // are exposed. 'onlyReadLatest' can also be set at the column family level.
  // However, the setting at this level takes precedence if 'onlyReadLatest' is
  // set at both levels.
  optional bool only_read_latest = 3 [json_name="onlyReadLatest"];

  // [Required] Qualifier of the column. Columns in the parent column family
  // that has this exact qualifier are exposed as . field. If the qualifier is
  // valid UTF-8 string, it can be specified in the qualifier_string field.
  // Otherwise, a base-64 encoded value must be set to qualifier_encoded. The
  // column field name is the same as the column qualifier. However, if the
  // qualifier is not a valid BigQuery field identifier i.e. does not match
  // a-zA-Z*, a valid identifier must be provided as field_name.
  optional string qualifier_encoded = 4 [json_name="qualifierEncoded"];

  // Qualifier string.
  optional string qualifier_string = 5 [json_name="qualifierString"];

  // Optional. The type to convert the value in cells of this column. The values
  // are expected to be encoded using HBase Bytes.toBytes function when using
  // the BINARY encoding value. Following BigQuery types are allowed
  // (case-sensitive): * BYTES * STRING * INTEGER * FLOAT * BOOLEAN * JSON
  // Default type is BYTES. 'type' can also be set at the column family level.
  // However, the setting at this level takes precedence if 'type' is set at
  // both levels.
  optional string type = 6 [json_name="type"];
}

// Information related to a Bigtable column family.
message BigtableColumnFamily {
  // Optional. Lists of columns that should be exposed as individual fields as
  // opposed to a list of (column name, value) pairs. All columns whose
  // qualifier matches a qualifier in this list can be accessed as .. Other
  // columns can be accessed as a list through .Column field.
  repeated BigtableColumn columns = 1 [json_name="columns"];

  // Optional. The encoding of the values when the type is not STRING.
  // Acceptable encoding values are: TEXT - indicates values are alphanumeric
  // text strings. BINARY - indicates values are encoded using HBase
  // Bytes.toBytes family of functions. This can be overridden for a specific
  // column by listing that column in 'columns' and specifying an encoding for
  // it.
  optional string encoding = 2 [json_name="encoding"];

  // Identifier of the column family.
  optional string family_id = 3 [json_name="familyId"];

  // Optional. If this is set only the latest version of value are exposed for
  // all columns in this column family. This can be overridden for a specific
  // column by listing that column in 'columns' and specifying a different
  // setting for that column.
  optional bool only_read_latest = 4 [json_name="onlyReadLatest"];

  // Optional. The type to convert the value in cells of this column family. The
  // values are expected to be encoded using HBase Bytes.toBytes function when
  // using the BINARY encoding value. Following BigQuery types are allowed
  // (case-sensitive): * BYTES * STRING * INTEGER * FLOAT * BOOLEAN * JSON
  // Default type is BYTES. This can be overridden for a specific column by
  // listing that column in 'columns' and specifying a type for it.
  optional string type = 5 [json_name="type"];
}

// Options specific to Google Cloud Bigtable data sources.
message BigtableOptions {
  // Optional. List of column families to expose in the table schema along with
  // their types. This list restricts the column families that can be referenced
  // in queries and specifies their value types. You can use this list to do
  // type conversions - see the 'type' field for more details. If you leave this
  // list empty, all column families are present in the table schema and their
  // values are read as BYTES. During a query only the column families
  // referenced in that query are read from Bigtable.
  repeated BigtableColumnFamily column_families = 1 [json_name="columnFamilies"];

  // Optional. If field is true, then the column families that are not specified
  // in columnFamilies list are not exposed in the table schema. Otherwise, they
  // are read with BYTES type values. The default value is false.
  optional bool ignore_unspecified_column_families = 2 [json_name="ignoreUnspecifiedColumnFamilies"];

  // Optional. If field is true, then each column family will be read as a
  // single JSON column. Otherwise they are read as a repeated cell structure
  // containing timestamp/value tuples. The default value is false.
  optional bool output_column_families_as_json = 3 [json_name="outputColumnFamiliesAsJson"];

  // Optional. If field is true, then the rowkey column families will be read
  // and converted to string. Otherwise they are read with BYTES type values and
  // users need to manually cast them with CAST if necessary. The default value
  // is false.
  optional bool read_rowkey_as_string = 4 [json_name="readRowkeyAsString"];
}

// Configures table clustering.
message Clustering {
  // One or more fields on which data should be clustered. Only top-level,
  // non-repeated, simple-type fields are supported. The ordering of the
  // clustering fields should be prioritized from most to least important for
  // filtering purposes. Additional information on limitations can be found
  // here:
  // https://cloud.google.com/bigquery/docs/creating-clustered-tables#limitations
  repeated string fields = 1 [json_name="fields"];
}

// Information related to a CSV data source.
message CsvOptions {
  // Optional. Indicates if BigQuery should accept rows that are missing
  // trailing optional columns. If true, BigQuery treats missing trailing
  // columns as null values. If false, records with missing trailing columns are
  // treated as bad records, and if there are too many bad records, an invalid
  // error is returned in the job result. The default value is false.
  optional bool allow_jagged_rows = 1 [json_name="allowJaggedRows"];

  // Optional. Indicates if BigQuery should allow quoted data sections that
  // contain newline characters in a CSV file. The default value is false.
  optional bool allow_quoted_newlines = 2 [json_name="allowQuotedNewlines"];

  // Optional. The character encoding of the data. The supported values are
  // UTF-8, ISO-8859-1, UTF-16BE, UTF-16LE, UTF-32BE, and UTF-32LE. The default
  // value is UTF-8. BigQuery decodes the data after the raw, binary data has
  // been split using the values of the quote and fieldDelimiter properties.
  optional string encoding = 3 [json_name="encoding"];

  // Optional. The separator character for fields in a CSV file. The separator
  // is interpreted as a single byte. For files encoded in ISO-8859-1, any
  // single character can be used as a separator. For files encoded in UTF-8,
  // characters represented in decimal range 1-127 (U+0001-U+007F) can be used
  // without any modification. UTF-8 characters encoded with multiple bytes
  // (i.e. U+0080 and above) will have only the first byte used for separating
  // fields. The remaining bytes will be treated as a part of the field.
  // BigQuery also supports the escape sequence "\t" (U+0009) to specify a tab
  // separator. The default value is comma (",", U+002C).
  optional string field_delimiter = 4 [json_name="fieldDelimiter"];

  // [Optional] A custom string that will represent a NULL value in CSV import
  // data.
  optional string null_marker = 5 [json_name="nullMarker"];

  // Optional. Indicates if the embedded ASCII control characters (the first 32
  // characters in the ASCII-table, from '\x00' to '\x1F') are preserved.
  optional bool preserve_ascii_control_characters = 6 [json_name="preserveAsciiControlCharacters"];

  // Optional. The value that is used to quote data sections in a CSV file.
  // BigQuery converts the string to ISO-8859-1 encoding, and then uses the
  // first byte of the encoded string to split the data in its raw, binary
  // state. The default value is a double-quote ("). If your data does not
  // contain quoted sections, set the property value to an empty string. If your
  // data contains quoted newline characters, you must also set the
  // allowQuotedNewlines property to true. To include the specific quote
  // character within a quoted value, precede it with an additional matching
  // quote character. For example, if you want to escape the default character '
  // " ', use ' "" '.
  optional string quote = 7 [json_name="quote"];

  // Optional. The number of rows at the top of a CSV file that BigQuery will
  // skip when reading the data. The default value is 0. This property is useful
  // if you have header rows in the file that should be skipped. When autodetect
  // is on, the behavior is the following: * skipLeadingRows unspecified -
  // Autodetect tries to detect headers in the first row. If they are not
  // detected, the row is read as data. Otherwise data is read starting from the
  // second row. * skipLeadingRows is 0 - Instructs autodetect that there are no
  // headers and data should be read starting from the first row. *
  // skipLeadingRows = N > 0 - Autodetect skips N-1 rows and tries to detect
  // headers in row N. If headers are not detected, row N is just skipped.
  // Otherwise row N is used to extract column names for the detected schema.
  optional string skip_leading_rows = 8 [json_name="skipLeadingRows"];
}

message ExternalDataConfiguration {
  // Try to detect schema and format options automatically. Any option specified
  // explicitly will be honored.
  optional bool autodetect = 1 [json_name="autodetect"];

  // Optional. Additional properties to set if sourceFormat is set to AVRO.
  optional AvroOptions avro_options = 2 [json_name="avroOptions"];

  // Optional. Additional options if sourceFormat is set to BIGTABLE.
  optional BigtableOptions bigtable_options = 3 [json_name="bigtableOptions"];

  // Optional. The compression type of the data source. Possible values include
  // GZIP and NONE. The default value is NONE. This setting is ignored for
  // Google Cloud Bigtable, Google Cloud Datastore backups, Avro, ORC and
  // Parquet formats. An empty string is an invalid value.
  optional string compression = 4 [json_name="compression"];

  // Optional. The connection specifying the credentials to be used to read
  // external storage, such as Azure Blob, Cloud Storage, or S3. The
  // connection_id can have the form
  // "<project\_id>.<location\_id>.<connection\_id>" or
  // "projects/<project\_id>/locations/<location\_id>/connections/<connection\_id>".
  optional string connection_id = 5 [json_name="connectionId"];

  // Optional. Additional properties to set if sourceFormat is set to CSV.
  optional CsvOptions csv_options = 6 [json_name="csvOptions"];

  // Defines the list of possible SQL data types to which the source decimal
  // values are converted. This list and the precision and the scale parameters
  // of the decimal field determine the target type. In the order of NUMERIC,
  // BIGNUMERIC, and STRING, a type is picked if it is in the specified list and
  // if it supports the precision and the scale. STRING supports all precision
  // and scale values. If none of the listed types supports the precision and
  // the scale, the type supporting the widest range in the specified list is
  // picked, and if a value exceeds the supported range when reading the data,
  // an error will be thrown. Example: Suppose the value of this field is
  // ["NUMERIC", "BIGNUMERIC"]. If (precision,scale) is: * (38,9) -> NUMERIC; *
  // (39,9) -> BIGNUMERIC (NUMERIC cannot hold 30 integer digits); * (38,10) ->
  // BIGNUMERIC (NUMERIC cannot hold 10 fractional digits); * (76,38) ->
  // BIGNUMERIC; * (77,38) -> BIGNUMERIC (error if value exeeds supported
  // range). This field cannot contain duplicate types. The order of the types
  // in this field is ignored. For example, ["BIGNUMERIC", "NUMERIC"] is the
  // same as ["NUMERIC", "BIGNUMERIC"] and NUMERIC always takes precedence over
  // BIGNUMERIC. Defaults to ["NUMERIC", "STRING"] for ORC and ["NUMERIC"] for
  // the other file formats.
  repeated string decimal_target_types = 7 [json_name="decimalTargetTypes"];

  // Optional. Specifies how source URIs are interpreted for constructing the
  // file set to load. By default source URIs are expanded against the
  // underlying storage. Other options include specifying manifest files. Only
  // applicable to object storage systems.
  // FILE_SET_SPEC_TYPE_FILE_SYSTEM_MATCH: This option expands source URIs by
  // listing files from the object store. It is the default behavior if
  // FileSetSpecType is not set.
  // FILE_SET_SPEC_TYPE_NEW_LINE_DELIMITED_MANIFEST: This option indicates that
  // the provided URIs are newline-delimited manifest files, with one URI per
  // line. Wildcard URIs are not supported.
  optional string file_set_spec_type = 8 [json_name="fileSetSpecType"];

  // Optional. Additional options if sourceFormat is set to GOOGLE_SHEETS.
  optional GoogleSheetsOptions google_sheets_options = 9 [json_name="googleSheetsOptions"];

  // Optional. When set, configures hive partitioning support. Not all storage
  // formats support hive partitioning -- requesting hive partitioning on an
  // unsupported format will lead to an error, as will providing an invalid
  // specification.
  optional HivePartitioningOptions hive_partitioning_options = 10 [json_name="hivePartitioningOptions"];

  // Optional. Indicates if BigQuery should allow extra values that are not
  // represented in the table schema. If true, the extra values are ignored. If
  // false, records with extra columns are treated as bad records, and if there
  // are too many bad records, an invalid error is returned in the job result.
  // The default value is false. The sourceFormat property determines what
  // BigQuery treats as an extra value: CSV: Trailing columns JSON: Named values
  // that don't match any column names Google Cloud Bigtable: This setting is
  // ignored. Google Cloud Datastore backups: This setting is ignored. Avro:
  // This setting is ignored. ORC: This setting is ignored. Parquet: This
  // setting is ignored.
  optional bool ignore_unknown_values = 11 [json_name="ignoreUnknownValues"];

  // Optional. Load option to be used together with source_format
  // newline-delimited JSON to indicate that a variant of JSON is being loaded.
  // To load newline-delimited GeoJSON, specify GEOJSON (and source_format must
  // be set to NEWLINE_DELIMITED_JSON).
  // JSON_EXTENSION_UNSPECIFIED: The default if provided value is not one
  // included in the enum, or the value is not specified. The source formate is
  // parsed without any modification.
  // GEOJSON: Use GeoJSON variant of JSON. See
  // https://tools.ietf.org/html/rfc7946.
  optional string json_extension = 12 [json_name="jsonExtension"];

  // Optional. Additional properties to set if sourceFormat is set to JSON.
  optional JsonOptions json_options = 13 [json_name="jsonOptions"];

  // Optional. The maximum number of bad records that BigQuery can ignore when
  // reading data. If the number of bad records exceeds this value, an invalid
  // error is returned in the job result. The default value is 0, which requires
  // that all records are valid. This setting is ignored for Google Cloud
  // Bigtable, Google Cloud Datastore backups, Avro, ORC and Parquet formats.
  optional int32 max_bad_records = 14 [json_name="maxBadRecords"];

  // Optional. Metadata Cache Mode for the table. Set this to enable caching of
  // metadata from external data source.
  // METADATA_CACHE_MODE_UNSPECIFIED: Unspecified metadata cache mode.
  // AUTOMATIC: Set this mode to trigger automatic background refresh of
  // metadata cache from the external source. Queries will use the latest
  // available cache version within the table's maxStaleness interval.
  // MANUAL: Set this mode to enable triggering manual refresh of the metadata
  // cache from external source. Queries will use the latest manually triggered
  // cache version within the table's maxStaleness interval.
  optional string metadata_cache_mode = 15 [json_name="metadataCacheMode"];

  // Optional. ObjectMetadata is used to create Object Tables. Object Tables
  // contain a listing of objects (with their metadata) found at the
  // source_uris. If ObjectMetadata is set, source_format should be omitted.
  // Currently SIMPLE is the only supported Object Metadata type.
  // OBJECT_METADATA_UNSPECIFIED: Unspecified by default.
  // DIRECTORY: A synonym for `SIMPLE`.
  // SIMPLE: Directory listing of objects.
  optional string object_metadata = 16 [json_name="objectMetadata"];

  // Optional. Additional properties to set if sourceFormat is set to PARQUET.
  optional ParquetOptions parquet_options = 17 [json_name="parquetOptions"];

  // Optional. When creating an external table, the user can provide a reference
  // file with the table schema. This is enabled for the following formats:
  // AVRO, PARQUET, ORC.
  optional string reference_file_schema_uri = 18 [json_name="referenceFileSchemaUri"];

  // Optional. The schema for the data. Schema is required for CSV and JSON
  // formats if autodetect is not on. Schema is disallowed for Google Cloud
  // Bigtable, Cloud Datastore backups, Avro, ORC and Parquet formats.
  optional TableSchema schema = 19 [json_name="schema"];

  // [Required] The data format. For CSV files, specify "CSV". For Google
  // sheets, specify "GOOGLE_SHEETS". For newline-delimited JSON, specify
  // "NEWLINE_DELIMITED_JSON". For Avro files, specify "AVRO". For Google Cloud
  // Datastore backups, specify "DATASTORE_BACKUP". For Apache Iceberg tables,
  // specify "ICEBERG". For ORC files, specify "ORC". For Parquet files, specify
  // "PARQUET". [Beta] For Google Cloud Bigtable, specify "BIGTABLE".
  optional string source_format = 20 [json_name="sourceFormat"];

  // [Required] The fully-qualified URIs that point to your data in Google
  // Cloud. For Google Cloud Storage URIs: Each URI can contain one '*' wildcard
  // character and it must come after the 'bucket' name. Size limits related to
  // load jobs apply to external data sources. For Google Cloud Bigtable URIs:
  // Exactly one URI can be specified and it has be a fully specified and valid
  // HTTPS URL for a Google Cloud Bigtable table. For Google Cloud Datastore
  // backups, exactly one URI can be specified. Also, the '*' wildcard character
  // is not allowed.
  repeated string source_uris = 21 [json_name="sourceUris"];
}

// Options specific to Google Sheets data sources.
message GoogleSheetsOptions {
  // Optional. Range of a sheet to query from. Only used when non-empty. Typical
  // format: sheet_name!top_left_cell_id:bottom_right_cell_id For example:
  // sheet1!A1:B20
  optional string range = 1 [json_name="range"];

  // Optional. The number of rows at the top of a sheet that BigQuery will skip
  // when reading the data. The default value is 0. This property is useful if
  // you have header rows that should be skipped. When autodetect is on, the
  // behavior is the following: * skipLeadingRows unspecified - Autodetect tries
  // to detect headers in the first row. If they are not detected, the row is
  // read as data. Otherwise data is read starting from the second row. *
  // skipLeadingRows is 0 - Instructs autodetect that there are no headers and
  // data should be read starting from the first row. * skipLeadingRows = N > 0
  // - Autodetect skips N-1 rows and tries to detect headers in row N. If
  // headers are not detected, row N is just skipped. Otherwise row N is used to
  // extract column names for the detected schema.
  optional string skip_leading_rows = 2 [json_name="skipLeadingRows"];
}

// Options for configuring hive partitioning detect.
message HivePartitioningOptions {
  // Output only. For permanent external tables, this field is populated with
  // the hive partition keys in the order they were inferred. The types of the
  // partition keys can be deduced by checking the table schema (which will
  // include the partition keys). Not every API will populate this field in the
  // output. For example, Tables.Get will populate it, but Tables.List will not
  // contain this field.
  repeated string fields = 1 [json_name="fields"];

  // Optional. When set, what mode of hive partitioning to use when reading
  // data. The following modes are supported: * AUTO: automatically infer
  // partition key name(s) and type(s). * STRINGS: automatically infer partition
  // key name(s). All types are strings. * CUSTOM: partition key schema is
  // encoded in the source URI prefix. Not all storage formats support hive
  // partitioning. Requesting hive partitioning on an unsupported format will
  // lead to an error. Currently supported formats are: JSON, CSV, ORC, Avro and
  // Parquet.
  optional string mode = 2 [json_name="mode"];

  // Optional. If set to true, queries over this table require a partition
  // filter that can be used for partition elimination to be specified. Note
  // that this field should only be true when creating a permanent external
  // table or querying a temporary external table. Hive-partitioned loads with
  // require_partition_filter explicitly set to true will fail.
  optional bool require_partition_filter = 3 [json_name="requirePartitionFilter"];

  // Optional. When hive partition detection is requested, a common prefix for
  // all source uris must be required. The prefix must end immediately before
  // the partition key encoding begins. For example, consider files following
  // this data layout:
  // gs://bucket/path_to_table/dt=2019-06-01/country=USA/id=7/file.avro
  // gs://bucket/path_to_table/dt=2019-05-31/country=CA/id=3/file.avro When hive
  // partitioning is requested with either AUTO or STRINGS detection, the common
  // prefix can be either of gs://bucket/path_to_table or
  // gs://bucket/path_to_table/. CUSTOM detection requires encoding the
  // partitioning schema immediately after the common prefix. For CUSTOM, any of
  // * gs://bucket/path_to_table/{dt:DATE}/{country:STRING}/{id:INTEGER} *
  // gs://bucket/path_to_table/{dt:STRING}/{country:STRING}/{id:INTEGER} *
  // gs://bucket/path_to_table/{dt:DATE}/{country:STRING}/{id:STRING} would all
  // be valid source URI prefixes.
  optional string source_uri_prefix = 4 [json_name="sourceUriPrefix"];
}

// Json Options for load and make external tables.
message JsonOptions {
  // Optional. The character encoding of the data. The supported values are
  // UTF-8, UTF-16BE, UTF-16LE, UTF-32BE, and UTF-32LE. The default value is
  // UTF-8.
  optional string encoding = 1 [json_name="encoding"];
}

// Parquet Options for load and make external tables.
message ParquetOptions {
  // Optional. Indicates whether to use schema inference specifically for
  // Parquet LIST logical type.
  optional bool enable_list_inference = 1 [json_name="enableListInference"];

  // Optional. Indicates whether to infer Parquet ENUM logical type as STRING
  // instead of BYTES by default.
  optional bool enum_as_string = 2 [json_name="enumAsString"];
}

message RangePartitioning {
  // Required. [Experimental] The table is partitioned by this field. The field
  // must be a top-level NULLABLE/REQUIRED field. The only supported type is
  // INTEGER/INT64.
  optional string field = 1 [json_name="field"];

  message Range {
    // [Experimental] The end of range partitioning, exclusive.
    optional string end = 1 [json_name="end"];

    // [Experimental] The width of each interval.
    optional string interval = 2 [json_name="interval"];

    // [Experimental] The start of range partitioning, inclusive.
    optional string start = 3 [json_name="start"];
  }

  // [Experimental] Defines the ranges for range partitioning.
  optional Range range = 2 [json_name="range"];
}

// A field in TableSchema
message TableFieldSchema {
  message Categories {
    // Deprecated.
    repeated string names = 1 [json_name="names"];
  }

  // Deprecated.
  optional Categories categories = 1 [json_name="categories"];

  // Optional. Field collation can be set only when the type of field is STRING.
  // The following values are supported: * 'und:ci': undetermined locale, case
  // insensitive. * '': empty string. Default to case-sensitive behavior.
  optional string collation = 2 [json_name="collation"];

  // Optional. A SQL expression to specify the [default value]
  // (https://cloud.google.com/bigquery/docs/default-values) for this field.
  optional string default_value_expression = 3 [json_name="defaultValueExpression"];

  // Optional. The field description. The maximum length is 1,024 characters.
  optional string description = 4 [json_name="description"];

  // Optional. Describes the nested schema fields if the type property is set to
  // RECORD.
  repeated TableFieldSchema fields = 5 [json_name="fields"];

  // Optional. Maximum length of values of this field for STRINGS or BYTES. If
  // max_length is not specified, no maximum length constraint is imposed on
  // this field. If type = "STRING", then max_length represents the maximum
  // UTF-8 length of strings in this field. If type = "BYTES", then max_length
  // represents the maximum number of bytes in this field. It is invalid to set
  // this field if type ≠ "STRING" and ≠ "BYTES".
  optional string max_length = 6 [json_name="maxLength"];

  // Optional. The field mode. Possible values include NULLABLE, REQUIRED and
  // REPEATED. The default value is NULLABLE.
  optional string mode = 7 [json_name="mode"];

  // Required. The field name. The name must contain only letters (a-z, A-Z),
  // numbers (0-9), or underscores (_), and must start with a letter or
  // underscore. The maximum length is 300 characters.
  optional string name = 8 [json_name="name"];

  message PolicyTags {
    // A list of policy tag resource names. For example,
    // "projects/1/locations/eu/taxonomies/2/policyTags/3". At most 1 policy tag
    // is currently allowed.
    repeated string names = 1 [json_name="names"];
  }

  // Optional. The policy tags attached to this field, used for field-level
  // access control. If not set, defaults to empty policy_tags.
  optional PolicyTags policy_tags = 9 [json_name="policyTags"];

  // Optional. Precision (maximum number of total digits in base 10) and scale
  // (maximum number of digits in the fractional part in base 10) constraints
  // for values of this field for NUMERIC or BIGNUMERIC. It is invalid to set
  // precision or scale if type ≠ "NUMERIC" and ≠ "BIGNUMERIC". If precision
  // and scale are not specified, no value range constraint is imposed on this
  // field insofar as values are permitted by the type. Values of this NUMERIC
  // or BIGNUMERIC field must be in this range when: * Precision (P) and scale
  // (S) are specified: [-10P-S + 10-S, 10P-S - 10-S] * Precision (P) is
  // specified but not scale (and thus scale is interpreted to be equal to
  // zero): [-10P + 1, 10P - 1]. Acceptable values for precision and scale if
  // both are specified: * If type = "NUMERIC": 1 ≤ precision - scale ≤ 29
  // and 0 ≤ scale ≤ 9. * If type = "BIGNUMERIC": 1 ≤ precision - scale
  // ≤ 38 and 0 ≤ scale ≤ 38. Acceptable values for precision if only
  // precision is specified but not scale (and thus scale is interpreted to be
  // equal to zero): * If type = "NUMERIC": 1 ≤ precision ≤ 29. * If type =
  // "BIGNUMERIC": 1 ≤ precision ≤ 38. If scale is specified but not
  // precision, then it is invalid.
  optional string precision = 10 [json_name="precision"];

  message RangeElementType {
    // Required. The type of a field element. See TableFieldSchema.type.
    optional string type = 1 [json_name="type"];
  }

  // Represents the type of a field element.
  optional RangeElementType range_element_type = 11 [json_name="rangeElementType"];

  // Optional. Specifies the rounding mode to be used when storing values of
  // NUMERIC and BIGNUMERIC type.
  // ROUNDING_MODE_UNSPECIFIED: Unspecified will default to using
  // ROUND_HALF_AWAY_FROM_ZERO.
  // ROUND_HALF_AWAY_FROM_ZERO: ROUND_HALF_AWAY_FROM_ZERO rounds half values
  // away from zero when applying precision and scale upon writing of NUMERIC
  // and BIGNUMERIC values. For Scale: 0 1.1, 1.2, 1.3, 1.4 => 1 1.5, 1.6, 1.7,
  // 1.8, 1.9 => 2
  // ROUND_HALF_EVEN: ROUND_HALF_EVEN rounds half values to the nearest even
  // value when applying precision and scale upon writing of NUMERIC and
  // BIGNUMERIC values. For Scale: 0 1.1, 1.2, 1.3, 1.4 => 1 1.5 => 2 1.6, 1.7,
  // 1.8, 1.9 => 2 2.5 => 2
  optional string rounding_mode = 12 [json_name="roundingMode"];

  // Optional. See documentation for precision.
  optional string scale = 13 [json_name="scale"];

  // Required. The field data type. Possible values include: * STRING * BYTES *
  // INTEGER (or INT64) * FLOAT (or FLOAT64) * BOOLEAN (or BOOL) * TIMESTAMP *
  // DATE * TIME * DATETIME * GEOGRAPHY * NUMERIC * BIGNUMERIC * JSON * RECORD
  // (or STRUCT) Use of RECORD/STRUCT indicates that the field contains a nested
  // schema.
  optional string type = 14 [json_name="type"];
}

// Schema of a table
message TableSchema {
  // Describes the fields in a table.
  repeated TableFieldSchema fields = 1 [json_name="fields"];
}

message TimePartitioning {
  // Optional. Number of milliseconds for which to keep the storage for a
  // partition. A wrapper is used here because 0 is an invalid value.
  optional string expiration_ms = 1 [json_name="expirationMs"];

  // Optional. If not set, the table is partitioned by pseudo column
  // '_PARTITIONTIME'; if set, the table is partitioned by this field. The field
  // must be a top-level TIMESTAMP or DATE field. Its mode must be NULLABLE or
  // REQUIRED. A wrapper is used here because an empty string is an invalid
  // value.
  optional string field = 2 [json_name="field"];

  // If set to true, queries over this table require a partition filter that can
  // be used for partition elimination to be specified. This field is
  // deprecated; please set the field with the same name on the table itself
  // instead. This field needs a wrapper because we want to output the default
  // value, false, if the user explicitly set it.
  optional bool require_partition_filter = 3 [json_name="requirePartitionFilter"];

  // Required. The supported types are DAY, HOUR, MONTH, and YEAR, which will
  // generate one partition per day, hour, month, and year, respectively.
  optional string type = 4 [json_name="type"];
}

//  This is used for defining User Defined Function (UDF) resources only when
// using legacy SQL. Users of GoogleSQL should leverage either DDL (e.g. CREATE
// [TEMPORARY] FUNCTION ... ) or the Routines API to define UDF resources. For
// additional information on migrating, see:
// https://cloud.google.com/bigquery/docs/reference/standard-sql/migrating-from-legacy-sql#differences_in_user-defined_javascript_functions
message UserDefinedFunctionResource {
  // [Pick one] An inline resource that contains code for a user-defined
  // function (UDF). Providing a inline code resource is equivalent to providing
  // a URI for a file containing the same code.
  optional string inline_code = 1 [json_name="inlineCode"];

  // [Pick one] A code resource to load from a Google Cloud Storage URI
  // (gs://bucket/path).
  optional string resource_uri = 2 [json_name="resourceUri"];
}
